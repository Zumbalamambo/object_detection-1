{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "from utils import train_utils\n",
    "from utils import googlenet_load\n",
    "\n",
    "import xml\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "%matplotlib inline\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import patches\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "from utils.data_utils import (annotation_jitter, image_to_h5, get_cell_grid)\n",
    "from utils.train_utils import add_rectangles\n",
    "from utils.annolist import AnnotationLib as al\n",
    "from utils.rect import Rect\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This system might perform poorly if the GoogleNet model was trained on data different than we're feeding.\n",
    "# In particular, different methods of cropping or resizing, BGR vs RGB, different whitening, etc could have \n",
    "# very large effects on how well this system works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = {\n",
    "    \"data\": {\n",
    "        \"train_idl\": \"./data/pascal/pascal_voc_train.idl\",\n",
    "        \"test_idl\": \"./data/pascal/pascal_voc_train.idl\",\n",
    "        \"truncate_data\": False   # Only loads first 10 files\n",
    "    },\n",
    "    \"arch\": {\n",
    "        \"use_lstm\": False,\n",
    "        \"use_dropout\": True, \n",
    "        \"image_width\": 640, \n",
    "        \"image_height\": 480,\n",
    "        \"grid_height\": 15, \n",
    "        \"grid_width\": 20,\n",
    "        \"batch_size\": 2, \n",
    "        \"region_size\": 32,\n",
    "        \"lstm_size\": 200,\n",
    "        \"num_classes\": 21,\n",
    "        \"rnn_len\": 1\n",
    "    }, \n",
    "    \"logging\": {\n",
    "    },\n",
    "    \"solver\": {\n",
    "        \"opt\": \"RMS\",\n",
    "        \"use_jitter\": False,\n",
    "        \"rnd_seed\": 1,\n",
    "        \"epsilon\": 0.00001, \n",
    "        \"learning_rate\": 0.001, \n",
    "        \"learning_rate_step\": 7500, \n",
    "        \"weights\": \"\",\n",
    "        \"head_weights\": [1.0, 0.1]   # The relative weighting of loss for bounding boxes and class predictions (not sure order)\n",
    "    }\n",
    "}\n",
    "H['solver']['gpu'] = 0\n",
    "H['exp_name'] = \"pascal\"\n",
    "H['save_dir'] = \"/home/ubuntu/external_drive/tensorbox_outputs\" + '/%s_%s' % (H['exp_name'],\n",
    "        datetime.datetime.now().strftime('%Y_%m_%d_%H.%M'))\n",
    "H['arch']['num_classes'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Makes a Rect object from my box class. Used to get the cool Rect methods. Confidence set to -1.\n",
    "def box_to_rect(box):\n",
    "    return Rect(box.xmin, box.ymin, box.xmax - box.xmin, box.ymax - box.ymin, confidence = -1)\n",
    "\n",
    "def box_to_anno_rect(box):\n",
    "    return al.AnnoRect(box.xmin, box.ymin, box.xmax, box.ymax)\n",
    "\n",
    "def annotation_to_h5(a, cell_width, cell_height, max_len):\n",
    "    region_size = 32\n",
    "    # Makes a cell_width x cell_height grid of region_size x region_size rects\n",
    "    cell_regions = get_cell_grid(cell_width, cell_height, region_size)\n",
    "\n",
    "    cells_per_image = len(cell_regions)\n",
    "    \n",
    "    # Makes a blank list of boxes \n",
    "    box_list = [[] for idx in range(cells_per_image)]\n",
    "            \n",
    "    for cidx, c in enumerate(cell_regions):\n",
    "        # put in box_list[cell_index] the list of all annotations that intersect somehow ??? with this box\n",
    "        box_list[cidx] = [box_to_anno_rect(r) for r in a[1] if all(box_to_anno_rect(r).intersection(c))]\n",
    "    \n",
    "    # a list of cells in the image\n",
    "    # [0, cell_id, :, box_id, 0] returns the box coords and size\n",
    "    boxes = np.zeros((1, cells_per_image, 4, max_len, 1), dtype = np.float)\n",
    "    \n",
    "    # a flag to indicate if there's an item in the box\n",
    "    box_flags = np.zeros((1, cells_per_image, 1, max_len, 1), dtype = np.float)\n",
    "    \n",
    "    # Iterate through each cell\n",
    "    for cidx in range(cells_per_image):\n",
    "        \n",
    "        # Get the number of classes in this cell. Since max_len is default 1, ...\n",
    "        cur_num_boxes = min(len(box_list[cidx]), max_len)\n",
    "        #assert(cur_num_boxes <= max_len)\n",
    "        \n",
    "        # Indicate that the first cur_num_boxes rows of the array have an associated box \n",
    "        box_flags[0, cidx, 0, 0:cur_num_boxes, 0] = 1\n",
    "        \n",
    "        # Get the coords of the center of the cell\n",
    "        cell_ox = 0.5*(cell_regions[cidx].x1 + cell_regions[cidx].x2)\n",
    "        cell_oy = 0.5*(cell_regions[cidx].y1 + cell_regions[cidx].y2)\n",
    "        \n",
    "        # create a list of the relative positions of annotation boxes within each cell\n",
    "        unsorted_boxes = []\n",
    "        for bidx in range(cur_num_boxes):\n",
    "\n",
    "            # relative box position with respect to cell\n",
    "            ox = 0.5 * (box_list[cidx][bidx].x1 + box_list[cidx][bidx].x2) - cell_ox\n",
    "            oy = 0.5 * (box_list[cidx][bidx].y1 + box_list[cidx][bidx].y2) - cell_oy\n",
    "\n",
    "            width = abs(box_list[cidx][bidx].x2 - box_list[cidx][bidx].x1)\n",
    "            height= abs(box_list[cidx][bidx].y2 - box_list[cidx][bidx].y1)\n",
    "\n",
    "            unsorted_boxes.append(np.array([ox, oy, width, height], dtype=np.float))\n",
    "        \n",
    "        # Sort boxes by distance from center of cell\n",
    "        for bidx, box in enumerate(sorted(unsorted_boxes, key=lambda x: x[0]**2 + x[1]**2)):\n",
    "            boxes[0, cidx, :, bidx, 0] = box\n",
    "    \n",
    "    # Returns data containing the relative positions of the annotation boxes within each cell in a grid\n",
    "    # and a flag indicating which cells actually have annotations within them\n",
    "    # boxes[0, cell_id, :, box_id, 0] returns the box coords and size\n",
    "    # box_flags[0, cell_id, 0, box_id, 0] returns whether or not there is actually a box at the given indices\n",
    "    return boxes, box_flags\n",
    "\n",
    "# Rescales an image and its bounding boxes to the target dimensions\n",
    "def rescale_boxes(I, anno, target_height, target_width):\n",
    "    x_scale = target_width / float(I.shape[1])\n",
    "    y_scale = target_height / float(I.shape[0])\n",
    "    this_anno = copy.deepcopy(anno)\n",
    "    for r in this_anno[1]:\n",
    "        #r = box_to_rect(r)\n",
    "        assert r.xmin < r.xmax\n",
    "        r.xmin *= x_scale\n",
    "        r.xmax *= x_scale\n",
    "        assert r.ymin < r.ymax\n",
    "        r.ymin *= y_scale\n",
    "        r.ymax *= y_scale\n",
    "    I_r = imresize(I, (target_height, target_width), interp='cubic')\n",
    "    return I_r, this_anno\n",
    "\n",
    "def load_idl_tf(H, jitter):\n",
    "    \"\"\"Take the idlfile and net configuration and create a generator\n",
    "    that outputs a jittered version of a random image from the annolist\n",
    "    that is mean corrected.\"\"\"\n",
    "\n",
    "    # Pull annotations from the global \"annotations\"\n",
    "    # annotations is a list of (filename, bounding_boxes)'s\n",
    "    annos = annotations\n",
    "    random.seed(0)\n",
    "    # Only use first 10 if truncate_data flag is set\n",
    "    #if H['data']['truncate_data']:\n",
    "    #    annos = annos[:10]\n",
    "    while True:\n",
    "        random.shuffle(annos)\n",
    "        for anno in annos:\n",
    "            # Load in the image contained in the filename\n",
    "            I = imread(anno[0])\n",
    "            if I.shape[0] != H[\"arch\"][\"image_height\"] or I.shape[1] != H[\"arch\"][\"image_width\"]:\n",
    "                I, anno = rescale_boxes(I, anno, H[\"arch\"][\"image_height\"], H[\"arch\"][\"image_width\"])\n",
    "            \"\"\"\n",
    "            if jitter:\n",
    "                jitter_scale_min=0.9\n",
    "                jitter_scale_max=1.1\n",
    "                jitter_offset=16\n",
    "                I, anno = annotation_jitter(I,\n",
    "                                            anno, target_width=H[\"arch\"][\"image_width\"],\n",
    "                                            target_height=H[\"arch\"][\"image_height\"],\n",
    "                                            jitter_scale_min=jitter_scale_min,\n",
    "                                            jitter_scale_max=jitter_scale_max,\n",
    "                                            jitter_offset=jitter_offset)\n",
    "            \"\"\"\n",
    "            # Idk what this shit is, and it'll definitely break..\n",
    "            boxes, flags = annotation_to_h5(anno,\n",
    "                                            H[\"arch\"][\"grid_width\"],\n",
    "                                            H[\"arch\"][\"grid_height\"],\n",
    "                                            H[\"arch\"][\"rnn_len\"])\n",
    "\n",
    "            yield {\"image\": I, \"boxes\": boxes, \"flags\": flags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test function for input generator\n",
    "def do_input_test(generator):\n",
    "    \n",
    "    data = next(generator)\n",
    "    image = data[\"image\"]\n",
    "    flags = data[\"flags\"]\n",
    "    boxes = data[\"boxes\"]\n",
    "    #plt.imshow(image)\n",
    "    image = image.reshape([1, 480, 640, 3])\n",
    "    boxes = boxes.reshape([300, 4])\n",
    "    flags = flags.reshape([300, 1])\n",
    "    confs = np.array([[make_sparse(detection, d=2) for detection in cell] for cell in flags])\n",
    "    confs = confs.reshape([300, 2])\n",
    "    # This probably overlays the rectange on the image???\n",
    "    test_output_to_log = train_utils.add_rectangles(image,\n",
    "                                                    confs,\n",
    "                                                    boxes,\n",
    "                                                    H[\"arch\"])[0]\n",
    "    #print(boxes)\n",
    "    plt.figure()\n",
    "    plt.imshow(test_output_to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a 1-hot encoding of n given total num classes d\n",
    "def make_sparse(n, d):\n",
    "    v = np.zeros((d,), dtype=np.float32)\n",
    "    v[n] = 1.\n",
    "    return v\n",
    "\n",
    "def load_data_gen(H, phase, jitter):\n",
    "    grid_size = H[\"arch\"]['grid_width'] * H[\"arch\"]['grid_height']\n",
    "\n",
    "    data = load_idl_tf(H, jitter={'train': jitter, 'test': False}[phase])\n",
    "\n",
    "    for d in data:\n",
    "        output = {}\n",
    "        \n",
    "        rnn_len = H[\"arch\"][\"rnn_len\"]\n",
    "        flags = d['flags'][0,:,0,0:rnn_len,0]\n",
    "        boxes = np.transpose(d['boxes'][0,:,:,0:rnn_len,0], (0,2,1))\n",
    "        assert(flags.shape == (grid_size, rnn_len))\n",
    "        assert(boxes.shape == (grid_size, rnn_len, 4))\n",
    "\n",
    "        output['image'] = d['image']\n",
    "        output['confs'] = np.array([[make_sparse(detection, d=2) for detection in cell] for cell in flags])\n",
    "        output['boxes'] = boxes\n",
    "        output['flags'] = flags\n",
    "        \n",
    "        yield output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How this network works:\n",
    "\n",
    "Architecture:\n",
    "    Standard conv-net. Final convolutional layer is connected to two fully-connected layers (in parallel, not series).\n",
    "    Confidences layer outputs a [num_classes] vector with predicted class confdiences for each position in the localization grid.\n",
    "    Bounding-box regressor layer outputs a regressed bounding box for each position in the grid.\n",
    "\n",
    "Training:\n",
    "    For a given training image, the localization grid is generated. For each position in the grid, a sparse vector \n",
    "    (like one-hot, but with a few places that can be \"hot\") is created that defines if a certain class is in that box in the image.\n",
    "    Also, for each occurance of a class, a \"regression target\" is created of where the object actually is within the grid box.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ops.RegisterGradient(\"Hungarian\")\n",
    "def _hungarian_grad(op, *args):\n",
    "    return map(array_ops.zeros_like, op.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_overfeat_forward(H, x, googlenet, phase):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        H: hyperparameters\n",
    "        x: inputs in some form???\n",
    "        googlenet: an initialized googlenet instance\n",
    "        phase: {train, test}\n",
    "    Returns:\n",
    "        pred_boxes: the op that predicts bounding boxes\n",
    "        pred_logits: the linear op that predicts classes\n",
    "        pred_confidences: the softmax class classifier op. Size [num_locations * batch_size, num_classes]\n",
    "    \"\"\"\n",
    "    # Subtract image mean from image\n",
    "    input_mean = 117.\n",
    "    x -= input_mean\n",
    "    \n",
    "    # Grab the last convolutional layer from a new GoogleNet graph\n",
    "    Z = googlenet_load.model(x, googlenet, H)\n",
    "    \n",
    "    grid_size = H['arch']['grid_width'] * H['arch']['grid_height']\n",
    "    \n",
    "    # If necessary, add a dropout layer to the graph\n",
    "    if H['arch']['use_dropout'] and phase == 'train':\n",
    "        Z = tf.nn.dropout(Z, 0.5)\n",
    "    \n",
    "    # Currently, W[0] is [1024, num_classes], and B is [num_classes]\n",
    "    # Z is [batch_size * grid_width * grid_height, 1024]\n",
    "    pred_logits = tf.matmul(Z, googlenet['W'][0]) + googlenet['B'][0] #, name=phase+'/logits_0')\n",
    "    # Reshape to [batch_size * grid_width * grid_height, num_classes]\n",
    "    # I think it's already this shape???\n",
    "    pred_logits = tf.reshape(pred_logits, [H['arch']['batch_size'] * grid_size, H['arch']['num_classes']])\n",
    "    # Do a softmax on this\n",
    "    # Result is, for each location, the confidence of each class\n",
    "    pred_confidences = tf.nn.softmax(pred_logits)\n",
    "    \n",
    "    # Now, do the same thing for the boxes\n",
    "    # WHAT THE HELL IS THE * 100 at the end???\n",
    "    # also, these reshapes make no sense - it's just un-reshaped in build_overfeat...\n",
    "    pred_boxes = tf.reshape(tf.nn.xw_plus_b(Z, googlenet['W'][1], googlenet['B'][1],\n",
    "                                            name=phase+'/logits_1'), \n",
    "                            [H['arch']['batch_size'] * grid_size, 1, 4]) * 100\n",
    "    return pred_boxes, pred_logits, pred_confidences\n",
    "\n",
    "\n",
    "def build_overfeat(H, x, googlenet, phase, boxes, confidences_r):\n",
    "    # Build the forward part\n",
    "    pred_boxes, pred_logits, pred_confidences = build_overfeat_forward(H, x, googlenet, phase)\n",
    "\n",
    "    grid_size = H['arch']['grid_width'] * H['arch']['grid_height']\n",
    "    \n",
    "    # Reshape boxes to [num_locations * batch_size, 4]\n",
    "    boxes = tf.cast(tf.reshape(boxes, [H['arch']['batch_size'] * grid_size, 4]), 'float32')\n",
    "    \n",
    "    # Calculate cross-entropy between real classes and predictions at each grid position\n",
    "    cross_entropy = -tf.reduce_sum(confidences_r*tf.log(tf.nn.softmax(pred_logits) + 1e-6))\n",
    "    \n",
    "    # Loss function based on difference between predicted boxes and actual boxes for boxes with an object\n",
    "    box_regressor_loss = tf.abs(pred_boxes[:, 0, :] - boxes) * tf.expand_dims(confidences_r[:, 1], 1)\n",
    "    \n",
    "    # Apply weights to losses \n",
    "    L = (H['solver']['head_weights'][0] * cross_entropy,\n",
    "         H['solver']['head_weights'][1] * box_regressor_loss)\n",
    "    \n",
    "    # Sum up weighted losses for confidences and normalize to batch and grid size\n",
    "    confidences_loss = (tf.reduce_sum(L[0], name=phase+'/confidences_loss') /\n",
    "                        (H['arch']['batch_size'] * grid_size))\n",
    "    \n",
    "    # Sum up weighted losses for boxes and normalize to batch and grid size\n",
    "    boxes_loss = (tf.reduce_sum(L[1], name=phase+'/boxes_loss') /\n",
    "                  (H['arch']['batch_size'] * grid_size))\n",
    "    \n",
    "    # Total loss\n",
    "    loss = confidences_loss + boxes_loss\n",
    "    \n",
    "    return pred_boxes, pred_confidences, loss, confidences_loss, boxes_loss\n",
    "\n",
    "def build(H, q):\n",
    "    '''\n",
    "    Build full model for training, including forward / backward passes,\n",
    "    optimizers, and summary statistics.\n",
    "    Inputs:\n",
    "        H: hyperparameter dict\n",
    "        q: dict with queues for train and test input examples\n",
    "    '''\n",
    "    arch = H['arch']\n",
    "    solver = H[\"solver\"]\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(solver['gpu'])\n",
    "\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    gpu_options = tf.GPUOptions()\n",
    "    config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "    \n",
    "    # Run googlenet_load init function\n",
    "    googlenet = googlenet_load.init(H, config)\n",
    "    \n",
    "    # Configure solver\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    if solver['opt'] == 'RMS':\n",
    "        opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                        decay=0.9, epsilon=solver['epsilon'])\n",
    "    elif solver['opt'] == 'SGD':\n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError('Unrecognized opt (optimizer) type')\n",
    "    \n",
    "    # We build two separate networks for training and testing!!! (is this necessary???)\n",
    "    loss, accuracy, confidences_loss, boxes_loss = {}, {}, {}, {}\n",
    "    for phase in ['train', 'test']:\n",
    "        # Get inputs from queue\n",
    "        x, confidences, boxes = q[phase].dequeue_many(arch['batch_size'])\n",
    "        \n",
    "        # ???\n",
    "        flags = tf.argmax(confidences, 3)\n",
    "\n",
    "\n",
    "        grid_size = H['arch']['grid_width'] * H['arch']['grid_height']\n",
    "        confidences_r = tf.cast(\n",
    "            tf.reshape(confidences[:, :, 0, :],\n",
    "                       [H['arch']['batch_size'] * grid_size, arch['num_classes']]), 'float32')\n",
    "        \n",
    "        (pred_boxes, pred_confidences,\n",
    "         loss[phase], confidences_loss[phase],\n",
    "         boxes_loss[phase]) = build_overfeat(H, x, googlenet, phase, boxes, confidences_r)\n",
    "\n",
    "\n",
    "        # Set up summary operations for tensorboard\n",
    "        a = tf.equal(tf.argmax(confidences_r, 1), tf.argmax(pred_confidences, 1))\n",
    "        accuracy[phase] = tf.reduce_mean(tf.cast(a, 'float32'), name=phase+'/accuracy')\n",
    "\n",
    "        if phase == 'train':\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            train_op = opt.minimize(loss['train'], global_step=global_step)\n",
    "        elif phase == 'test':\n",
    "            test_image = x\n",
    "            moving_avg = tf.train.ExponentialMovingAverage(0.99)\n",
    "            smooth_op = moving_avg.apply([accuracy['train'], accuracy['test'],\n",
    "                                          confidences_loss['train'], boxes_loss['train'],\n",
    "                                          confidences_loss['test'], boxes_loss['test'],\n",
    "                                          ])\n",
    "\n",
    "            for p in ['train', 'test']:\n",
    "                tf.scalar_summary('%s/accuracy' % p, accuracy[p])\n",
    "                tf.scalar_summary('%s/accuracy/smooth' % p, moving_avg.average(accuracy[p]))\n",
    "                tf.scalar_summary(\"%s/confidences_loss\" % p, confidences_loss[p])\n",
    "                tf.scalar_summary(\"%s/confidences_loss/smooth\" % p,\n",
    "                    moving_avg.average(confidences_loss[p]))\n",
    "                tf.scalar_summary(\"%s/regression_loss\" % p, boxes_loss[p])\n",
    "                tf.scalar_summary(\"%s/regression_loss/smooth\" % p,\n",
    "                    moving_avg.average(boxes_loss[p]))\n",
    "\n",
    "            # show ground truth to verify labels are correct\n",
    "            test_true_confidences = confidences_r\n",
    "            test_true_boxes = boxes[0, :, 0, :]\n",
    "\n",
    "            # show predictions to visualize training progress\n",
    "            test_pred_confidences = pred_confidences\n",
    "            test_pred_boxes = pred_boxes[:, 0, :]\n",
    "\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    return (config, loss, accuracy, summary_op, train_op, googlenet['W_norm'],\n",
    "            test_image, test_pred_boxes, test_pred_confidences,\n",
    "            test_true_boxes, test_true_confidences, smooth_op,\n",
    "            global_step, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PASCAL VOC Data import\n",
    "home_dir = os.path.expanduser('~') + \"/\"\n",
    "datasets_dir = home_dir + \"external_drive/\"\n",
    "voc_2012_dir = datasets_dir + \"VOC/VOC2012/VOCdevkit/VOC2012/\"\n",
    "\n",
    "classes_to_index = {\"aeroplane\":1, \"bicycle\":2, \"boat\":3, \"bottle\":4, \"bus\":5, \"car\":6, \"cat\":7,\n",
    "\"chair\":8, \"cow\":9, \"diningtable\":10, \"dog\":11, \"horse\":12, \"motorbike\":13, \"person\":14,\n",
    "\"pottedplant\":15, \"sheep\":16, \"train\":17, \"tvmonitor\":18, \"sofa\":19, \"bird\":20}\n",
    "\n",
    "indexes_to_classes = [0]*21\n",
    "for key, value in classes_to_index.items():\n",
    "    indexes_to_classes[value] = key\n",
    "\n",
    "class bounding_box:\n",
    "    def __init__(self, class_name, xmin, xmax, ymin, ymax):\n",
    "        self.class_name = class_name\n",
    "        #self.bbox_coords = bbox_coords #xmax, xmin, ymax, ymin\n",
    "        self.xmin = xmin\n",
    "        self.xmax = xmax\n",
    "        self.ymin = ymin\n",
    "        self.ymax = ymax\n",
    "\n",
    "# Get list of training images\n",
    "train_filenames = []\n",
    "with open(voc_2012_dir + \"ImageSets/Main/train.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        train_filenames.append(line.strip())\n",
    "        \n",
    "# Read an annotation file, return a list of bounding boxes\n",
    "def get_bounding_boxes(filename):\n",
    "    bounding_boxes = []\n",
    "    xmldoc = ET.parse(voc_2012_dir + \"Annotations/{}.xml\".format(filename))\n",
    "    objects = xmldoc.findall('object')\n",
    "    for thing in objects:\n",
    "        name = thing.find('name').text\n",
    "        box = thing.find('bndbox')\n",
    "        xmax = int(box.find('xmax').text)\n",
    "        xmin = int(box.find('xmin').text)\n",
    "        ymax = int(box.find('ymax').text)\n",
    "        ymin = int(box.find('ymin').text)\n",
    "        bounding_boxes.append(bounding_box(name, xmin, xmax, ymin, ymax))\n",
    "    return bounding_boxes\n",
    "\n",
    "# Make a list of annotations in the format (filename, bounding_boxes)\n",
    "annotations = []\n",
    "for filename in train_filenames:\n",
    "    full_filename = voc_2012_dir + \"JPEGImages/{}.jpg\".format(filename)\n",
    "    bounding_boxes = get_bounding_boxes(filename)\n",
    "    annotations.append((full_filename, bounding_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_saves = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if do_saves:\n",
    "    # Make sure directory for saving model checkpoints and data exists\n",
    "    if not os.path.exists(H['save_dir']): \n",
    "        os.makedirs(H['save_dir'])\n",
    "\n",
    "    # Dump hyperparameters to file for reference\n",
    "    ckpt_file = H['save_dir'] + '/save.ckpt'\n",
    "    with open(H['save_dir'] + '/hypes.json', 'w') as f:\n",
    "        json.dump(H, f, indent=4)\n",
    "\n",
    "# Set up placeholder ops for inputs\n",
    "x_in = tf.placeholder(tf.float32)\n",
    "confs_in = tf.placeholder(tf.float32)\n",
    "boxes_in = tf.placeholder(tf.float32)\n",
    "\n",
    "# Queue to hold train and test inputs - (x_in, confs_in, boxes_in)\n",
    "q = {}\n",
    "# Enqueue op for train or test queues\n",
    "enqueue_op = {}\n",
    "for phase in ['train', 'test']:\n",
    "    # Each queue holds 3 tensors - each of type tf.float32\n",
    "    dtypes = [tf.float32, tf.float32, tf.float32]\n",
    "    # Set up a (default 15x20 grid) to do ???\n",
    "    grid_size = H['arch']['grid_width'] * H['arch']['grid_height']\n",
    "    shapes = (\n",
    "        [H['arch']['image_height'], H['arch']['image_width'], 3], # Size of input image\n",
    "        [grid_size, H['arch']['rnn_len'], H['arch']['num_classes']], # ???\n",
    "        [grid_size, H['arch']['rnn_len'], 4], # ???\n",
    "        )\n",
    "    q[phase] = tf.FIFOQueue(capacity=30, dtypes=dtypes, shapes=shapes)\n",
    "    enqueue_op[phase] = q[phase].enqueue((x_in, confs_in, boxes_in))\n",
    "\n",
    "# Make a dictionary to feed into the training step of tensorflow\n",
    "def make_feed(d):\n",
    "    return {x_in: d['image'], confs_in: d['confs'], boxes_in: d['boxes'],\n",
    "            learning_rate: H['solver']['learning_rate']}\n",
    "\n",
    "# A loop that enqueues data - to be run in its own thread\n",
    "# Phase is {train, test}\n",
    "# gen is a generator for examples to enqueue\n",
    "def MyLoop(sess, enqueue_op, phase, gen):\n",
    "    for d in gen:\n",
    "        sess.run(enqueue_op[phase], feed_dict=make_feed(d))\n",
    "\n",
    "# Build the network! Returns a bunch of useful ops\n",
    "(config, loss, accuracy, summary_op, train_op, W_norm,\n",
    " test_image, test_pred_boxes, test_pred_confidences,\n",
    " test_true_boxes, test_true_confidences,\n",
    " smooth_op, global_step, learning_rate) = build(H, q)\n",
    "\n",
    "\n",
    "# Create ops to log an input image, if given to fill placeholder test_image_to_log\n",
    "test_image_to_log = tf.placeholder(tf.uint8,\n",
    "                                   [H['arch']['image_height'], H['arch']['image_width'], 3])\n",
    "log_image_name = tf.placeholder(tf.string)\n",
    "log_image = tf.image_summary(log_image_name, tf.expand_dims(test_image_to_log, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#derp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a saver\n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "writer = tf.train.SummaryWriter(\n",
    "    logdir=H['save_dir'], \n",
    "    graph_def=sess.graph_def,\n",
    "    flush_secs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/standard/lib/python3.4/site-packages/ipykernel/__main__.py:4: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# Start threads to keep queues filled with input examples\n",
    "threads = []\n",
    "for phase in ['train', 'test']:\n",
    "    # enqueue once manually to avoid thread start delay\n",
    "    gen = load_data_gen(H, phase, jitter=H['solver']['use_jitter'])\n",
    "    d = next(gen)\n",
    "    sess.run(enqueue_op[phase], feed_dict=make_feed(d))\n",
    "    \n",
    "    # Create a thread to enqueue data\n",
    "    threads.append(tf.train.threading.Thread(target=MyLoop,\n",
    "                                             args=(sess, enqueue_op, phase, gen)))\n",
    "    threads[-1].start()\n",
    "\n",
    "# Set tensorflow random seed from hyperparameter config\n",
    "tf.set_random_seed(H['solver']['rnd_seed'])\n",
    "\n",
    "# Initialize variables\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/standard/lib/python3.4/site-packages/ipykernel/__main__.py:4: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# restore from checkpoint\n",
    "path_to_checkpoint = \"/home/ubuntu/packages/tensorbox/pascal_2016_03_13_06.40/save.ckpt-17000\"\n",
    "saver.restore(sess, path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train model for N iterations\n",
    "start = time.time()\n",
    "\n",
    "for i in range(34000, 1000000):\n",
    "    # Evaluate the network every display_iter iterations\n",
    "    display_iter = 10\n",
    "    \n",
    "    # Learning rate decay function\n",
    "    adjusted_lr = (H['solver']['learning_rate'] *\n",
    "                   0.5 ** max(0, (i / H['solver']['learning_rate_step']) - 2))\n",
    "    lr_feed = {learning_rate: adjusted_lr}\n",
    "    \n",
    "    if i % display_iter == 0:\n",
    "        if i > 0:\n",
    "            # Print timings\n",
    "            dt = (time.time() - start) / (H['arch']['batch_size'] * display_iter)\n",
    "        \n",
    "        # Reset timer\n",
    "        start = time.time()\n",
    "        \n",
    "        # Run network and get lots of things back\n",
    "        (batch_loss_train, test_accuracy, weights_norm, \n",
    "            summary_str, np_test_image, np_test_pred_boxes,\n",
    "            np_test_pred_confidences, np_test_true_boxes,\n",
    "            np_test_true_confidences, _, _) = sess.run([\n",
    "                 loss['train'], accuracy['test'], W_norm, \n",
    "                 summary_op, test_image, test_pred_boxes,\n",
    "                 test_pred_confidences, test_true_boxes, test_true_confidences,\n",
    "                 train_op, smooth_op,\n",
    "                ], feed_dict=lr_feed)\n",
    "        \n",
    "        \n",
    "        pred_true = [(\"%d_pred_output\" % (i % 3), np_test_pred_boxes, np_test_pred_confidences),\n",
    "                     (\"%d_true_output\" % (i % 3), np_test_true_boxes, np_test_true_confidences)]\n",
    "        \n",
    "        # For each box in the image (both real and predicted), create a summary image\n",
    "        # with the box overlaid on it\n",
    "        for name, boxes, confidences in pred_true:\n",
    "            # This probably overlays the rectange on the image???\n",
    "            test_output_to_log = train_utils.add_rectangles(np_test_image,\n",
    "                                                            confidences,\n",
    "                                                            boxes,\n",
    "                                                            H[\"arch\"])[0]\n",
    "            \n",
    "            # Check that test_output_to_log is indeed an image\n",
    "            assert test_output_to_log.shape == (H['arch']['image_height'],\n",
    "                                                H['arch']['image_width'], 3)\n",
    "            \n",
    "            # Feed the image into its placeholder\n",
    "            feed = {test_image_to_log: test_output_to_log, log_image_name: name}\n",
    "            \n",
    "            # Get and add the image summary\n",
    "            test_image_summary_str = sess.run(log_image, feed_dict=feed)\n",
    "            if do_saves: writer.add_summary(test_image_summary_str, global_step=global_step.eval())\n",
    "        \n",
    "        # Add the real summary\n",
    "        if do_saves: writer.add_summary(summary_str, global_step=global_step.eval())\n",
    "        \n",
    "        # Print progress update\n",
    "        print_str = ', '.join([\n",
    "            'Step: %d',\n",
    "            'lr: %f',\n",
    "            'Train Loss: %.2f',\n",
    "            'Test Accuracy: %.1f%%',\n",
    "            'Time/image (ms): %.1f'\n",
    "        ])\n",
    "        print(print_str % \n",
    "              (i, adjusted_lr, batch_loss_train,\n",
    "               test_accuracy * 100, dt * 1000 if i > 0 else 0))\n",
    "    else:\n",
    "        # Run the training step\n",
    "        batch_loss_train, _ = sess.run([loss['train'], train_op], feed_dict=lr_feed)\n",
    "    \n",
    "    # Every 1000 steps, save a checkpoint\n",
    "    if global_step.eval() % 1000 == 0: \n",
    "        if do_saves: saver.save(sess, ckpt_file, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_pred = True\n",
    "for _ in range(10):\n",
    "    lr_feed = {learning_rate: .1}\n",
    "    if do_pred:\n",
    "        image, boxes, confidences = sess.run([test_image, test_pred_boxes, test_pred_confidences], feed_dict=lr_feed)\n",
    "    else:\n",
    "        image, boxes, confidences = sess.run([test_image, test_true_boxes, test_true_confidences], feed_dict=lr_feed)\n",
    "    # This probably overlays the rectange on the image???\n",
    "    test_output_to_log = train_utils.add_rectangles(image,\n",
    "                                                    confidences,\n",
    "                                                    boxes,\n",
    "                                                    H[\"arch\"])[0]\n",
    "    #print(boxes)\n",
    "    plt.figure()\n",
    "    plt.imshow(test_output_to_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
